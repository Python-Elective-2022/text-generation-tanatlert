{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da88478-e5c1-4f1f-ba59-2a4422ce580c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-27 04:31:03--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 209.85.200.101, 209.85.200.100, 209.85.200.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|209.85.200.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rb3cfe9n66bu92kf55ivffh1mbgn8a4s/1679891400000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=226ee4ed-e7c1-40d5-95fc-74c7b0e5b2b2 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-27 04:31:05--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rb3cfe9n66bu92kf55ivffh1mbgn8a4s/1679891400000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=226ee4ed-e7c1-40d5-95fc-74c7b0e5b2b2\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.69.132, 2607:f8b0:4001:c08::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.69.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   257MB/s    in 0.3s    \n",
            "\n",
            "2023-03-27 04:31:06 (257 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b181e65-a7d4-4c99-e8cb-3e3c87ac0971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f57e6f-98ef-4022-80e3-248e4a39bdcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa68814-b34b-4306-8c06-fd0b441094dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "62/62 [==============================] - 7s 53ms/step - loss: 5.8063 - accuracy: 0.0328\n",
            "Epoch 2/100\n",
            "62/62 [==============================] - 2s 30ms/step - loss: 5.4199 - accuracy: 0.0338\n",
            "Epoch 3/100\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 5.3297 - accuracy: 0.0358\n",
            "Epoch 4/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.1936 - accuracy: 0.0439\n",
            "Epoch 5/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.0302 - accuracy: 0.0621\n",
            "Epoch 6/100\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.8610 - accuracy: 0.0777\n",
            "Epoch 7/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.6963 - accuracy: 0.0893\n",
            "Epoch 8/100\n",
            "62/62 [==============================] - 1s 7ms/step - loss: 4.5078 - accuracy: 0.1135\n",
            "Epoch 9/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3246 - accuracy: 0.1312\n",
            "Epoch 10/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1248 - accuracy: 0.1599\n",
            "Epoch 11/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.9295 - accuracy: 0.1917\n",
            "Epoch 12/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7523 - accuracy: 0.2230\n",
            "Epoch 13/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5917 - accuracy: 0.2523\n",
            "Epoch 14/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4400 - accuracy: 0.2820\n",
            "Epoch 15/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.2668 - accuracy: 0.3269\n",
            "Epoch 16/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0985 - accuracy: 0.3587\n",
            "Epoch 17/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.9539 - accuracy: 0.3870\n",
            "Epoch 18/100\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.8549 - accuracy: 0.4137\n",
            "Epoch 19/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7070 - accuracy: 0.4344\n",
            "Epoch 20/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5668 - accuracy: 0.4571\n",
            "Epoch 21/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4691 - accuracy: 0.4808\n",
            "Epoch 22/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3616 - accuracy: 0.4945\n",
            "Epoch 23/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.2785 - accuracy: 0.5111\n",
            "Epoch 24/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.1677 - accuracy: 0.5368\n",
            "Epoch 25/100\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 2.0709 - accuracy: 0.5610\n",
            "Epoch 26/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.9882 - accuracy: 0.5848\n",
            "Epoch 27/100\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9177 - accuracy: 0.5883\n",
            "Epoch 28/100\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.8389 - accuracy: 0.5994\n",
            "Epoch 29/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7452 - accuracy: 0.6251\n",
            "Epoch 30/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6645 - accuracy: 0.6458\n",
            "Epoch 31/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5939 - accuracy: 0.6574\n",
            "Epoch 32/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5352 - accuracy: 0.6756\n",
            "Epoch 33/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4627 - accuracy: 0.6877\n",
            "Epoch 34/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4568 - accuracy: 0.6816\n",
            "Epoch 35/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3800 - accuracy: 0.7094\n",
            "Epoch 36/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3153 - accuracy: 0.7275\n",
            "Epoch 37/100\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.3680 - accuracy: 0.7028\n",
            "Epoch 38/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2953 - accuracy: 0.7245\n",
            "Epoch 39/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1962 - accuracy: 0.7578\n",
            "Epoch 40/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1237 - accuracy: 0.7745\n",
            "Epoch 41/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0911 - accuracy: 0.7836\n",
            "Epoch 42/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0559 - accuracy: 0.7921\n",
            "Epoch 43/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0453 - accuracy: 0.7916\n",
            "Epoch 44/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0400 - accuracy: 0.7916\n",
            "Epoch 45/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9694 - accuracy: 0.8088\n",
            "Epoch 46/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9117 - accuracy: 0.8174\n",
            "Epoch 47/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8745 - accuracy: 0.8269\n",
            "Epoch 48/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.8336 - accuracy: 0.8380\n",
            "Epoch 49/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.8096 - accuracy: 0.8396\n",
            "Epoch 50/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7689 - accuracy: 0.8431\n",
            "Epoch 51/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7446 - accuracy: 0.8542\n",
            "Epoch 52/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7264 - accuracy: 0.8567\n",
            "Epoch 53/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7157 - accuracy: 0.8552\n",
            "Epoch 54/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6979 - accuracy: 0.8532\n",
            "Epoch 55/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6588 - accuracy: 0.8683\n",
            "Epoch 56/100\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6388 - accuracy: 0.8673\n",
            "Epoch 57/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6403 - accuracy: 0.8663\n",
            "Epoch 58/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6092 - accuracy: 0.8718\n",
            "Epoch 59/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5930 - accuracy: 0.8744\n",
            "Epoch 60/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5894 - accuracy: 0.8749\n",
            "Epoch 61/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5651 - accuracy: 0.8789\n",
            "Epoch 62/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5374 - accuracy: 0.8835\n",
            "Epoch 63/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5327 - accuracy: 0.8885\n",
            "Epoch 64/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5105 - accuracy: 0.8930\n",
            "Epoch 65/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4979 - accuracy: 0.8845\n",
            "Epoch 66/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4815 - accuracy: 0.8900\n",
            "Epoch 67/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4678 - accuracy: 0.8900\n",
            "Epoch 68/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4648 - accuracy: 0.8915\n",
            "Epoch 69/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4767 - accuracy: 0.8870\n",
            "Epoch 70/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4450 - accuracy: 0.8935\n",
            "Epoch 71/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4291 - accuracy: 0.8996\n",
            "Epoch 72/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4176 - accuracy: 0.8996\n",
            "Epoch 73/100\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.4175 - accuracy: 0.8961\n",
            "Epoch 74/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3981 - accuracy: 0.8951\n",
            "Epoch 75/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3850 - accuracy: 0.9041\n",
            "Epoch 76/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3808 - accuracy: 0.9011\n",
            "Epoch 77/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3737 - accuracy: 0.9011\n",
            "Epoch 78/100\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3654 - accuracy: 0.9072\n",
            "Epoch 79/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3590 - accuracy: 0.9031\n",
            "Epoch 80/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4065 - accuracy: 0.8925\n",
            "Epoch 81/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4124 - accuracy: 0.8925\n",
            "Epoch 82/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3718 - accuracy: 0.9041\n",
            "Epoch 83/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3491 - accuracy: 0.9087\n",
            "Epoch 84/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3432 - accuracy: 0.9036\n",
            "Epoch 85/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3328 - accuracy: 0.9072\n",
            "Epoch 86/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3264 - accuracy: 0.9031\n",
            "Epoch 87/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3214 - accuracy: 0.9006\n",
            "Epoch 88/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3159 - accuracy: 0.9067\n",
            "Epoch 89/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3125 - accuracy: 0.9062\n",
            "Epoch 90/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3037 - accuracy: 0.9062\n",
            "Epoch 91/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3023 - accuracy: 0.9102\n",
            "Epoch 92/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3033 - accuracy: 0.9031\n",
            "Epoch 93/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2916 - accuracy: 0.9041\n",
            "Epoch 94/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2865 - accuracy: 0.9036\n",
            "Epoch 95/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2940 - accuracy: 0.9036\n",
            "Epoch 96/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2872 - accuracy: 0.9092\n",
            "Epoch 97/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2949 - accuracy: 0.9031\n",
            "Epoch 98/100\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3135 - accuracy: 0.8991\n",
            "Epoch 99/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3238 - accuracy: 0.8966\n",
            "Epoch 100/100\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2955 - accuracy: 0.8971\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(60)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a098143e-de69-4a22-dfa3-e18f33caf976"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba78989-27a1-42cb-a733-91b86e583f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 655ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "crunchy roll take me home to my pain will end me strong strong strong loves good care care think morning walls warm late late city fool misunderstood easy think think late its taking think think youll youll\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"crunchy roll take me home to my\"\n",
        "next_words = 30\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}